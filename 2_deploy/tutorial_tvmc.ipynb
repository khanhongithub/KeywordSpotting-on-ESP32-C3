{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77205ad4-e375-41e4-b579-08f7cf8da8e5",
   "metadata": {},
   "source": [
    "# MircoKWS deployment using TVMC command-line interface\n",
    "\n",
    "This document will explain the required steps to process a model using the TVM machine learning compiler framework in detail. To generate the inference code for the application example of real-time keyword-spotting, the following steps have to be performed.\n",
    "\n",
    "*While the following steps should work on modern versions of Windows, MacOS and Ubuntu, the complete flow was only tested on Ubuntu.*\n",
    "\n",
    "**Before continuing: Make sure that your virtual environment from the TVM installation step is active (sourced).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4ab23-13fb-4da8-8d8f-faba3ed3d194",
   "metadata": {},
   "source": [
    "### Obtaining a model\n",
    "\n",
    "The examples are intended to be used with an eight keyword model `data/micro_kws_student_quantized.tflite` (Words: **yes,no,up,down,left,right,on,off**) from lab 1, which you find in the `tvm/data/` directory. However, most steps should also be applicable to any other model. A very small example model using only two keywords is also provided: `data/micro_kws_xs_quantized.tflite` (Words: **yes,no**)\n",
    "\n",
    "Various \"Model-Zoos\" are available on the internet if you want to use existing pre-trained models for a given dataset/application. We also provide our chair's set of TinyML benchmarking models in a GitHub repository: https://github.com/tum-ei-eda/mlonmcu-models.\n",
    "\n",
    "Beside of `.tflite` files, TVM also supports various other model formats such as ONNX. However, only quantized TFLite files will be considered in this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38726917-79cf-4513-ac0f-a2c37792ad22",
   "metadata": {},
   "source": [
    "### Compiling the model\n",
    "\n",
    "The term `compile` in the context of TVMC describes the complete compilation pipeline internally used by TVM (e.g. Relay frontend, Partitioning, Lowering, Code Generation etc.).\n",
    "\n",
    "In this section, an example model is processed via the TVMC command line interface for two typical application scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80842a3a-bc82-47a7-86c8-cdd5e796aa5f",
   "metadata": {},
   "source": [
    "#### For execution on host\n",
    "\n",
    "The most straightforward way to get started with TVM is using the plain `llvm` target. A a later point in time we can add additional information to our target, to make use of target-specific optimizations which are implemented in TVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f9e5ca-6d77-4c96-8127-9830d001b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gen\n",
    "!tvmc compile data/micro_kws_student_quantized.tflite --output gen/module.tar \\\n",
    "    --target llvm --desired-layout NCHW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985bc67-c46f-4ad8-b9a6-4f7f34565f81",
   "metadata": {},
   "source": [
    "The used command line options can be explained as follows:\n",
    "- `data/micro_kws_student_quantized.tflite`: The (quantized) TFLite model to process\n",
    "- `--output gen/module.tar`: The destination archive which will contain a shared object library\n",
    "- `--target llvm`: Tell TVM that we want to use the LLVM backend\n",
    "- `--desired-layout {NCHW,NHWC}`: Set the preferred layout of weights/kernels in the model (optional)\n",
    "\n",
    "Further information on the available options can be found using the `--help` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8513862-5ce7-4c90-a8f2-da425798e274",
   "metadata": {},
   "source": [
    "#### For execution on embedded device\n",
    "\n",
    "The following (quite complex) command should be used to generate the TVM kernel implementations used in a later step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75866cb0-52f1-457b-9943-914ec142add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gen\n",
    "!tvmc compile data/micro_kws_student_quantized.tflite --output gen/mlf.tar \\\n",
    "    --target c --runtime crt --executor-aot-interface-api c \\\n",
    "    --executor aot --executor-aot-unpacked-api 1 --desired-layout NCHW \\\n",
    "    --output-format mlf --pass-config tir.disable_vectorize=1 \\\n",
    "    --pass-config tir.usmp.enable=1 --pass-config tir.usmp.algorithm=hill_climb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93f90c-c2a5-4e7a-974e-b6f961a18efa",
   "metadata": {},
   "source": [
    "The used command line options can be explained as follows:\n",
    "- `data/micro_kws_student.tflite`: The (quantized) TFLite model to process\n",
    "- `--output gen/mlf.tar`: The destination file\n",
    "- `--target c`: Tell TVM that we want to generate C kernels and not LLVM\n",
    "- `--runtime crt`: Use the standalone CRT as we want to use a minimal runtime environment (e.g. baremetal code)\n",
    "- `--executor-aot-interface-api c`: Generate a straightforward interface to define input and output tensors.\n",
    "- `--executor aot`: Generate top-level model code using the Ahead-of-Time compiler to get rid of any runtime/interpreter related-overheads (Alternative: `graph` runtime using `graph.json` and `params.bin`)\n",
    "- `--executor-aot-unpacked-api 1`: Use the \\\"unpacked\\\" calling convention for more compact code and less stack usage compared to TVM's default approach.\n",
    "- `--desired-layout {NCHW,NHWC}`: Set the preferred layout of weights/kernels in the model (optional)\n",
    "- `--output-format mlf`: Return MLF archive with codegen results. (Explained later)\n",
    "- `--pass-config tir.disable_vectorize=1`: Disable optimizations which are not available on embedded platforms\n",
    "- `--pass-config tir.usmp.enable=1`: Use the USMP (Unified Static Memory Planner) to minimize memory usage using a global tensor arena.\n",
    "- `--pass-config tir.usmp.algorithm=hill_climb` Select the algorithm used by the USMP (Alternatives: `greedy_by_size`, `greedy_by_conflicts`)\n",
    "\n",
    "Further information on the available options can be found using the `--help` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e3e30-ca47-4167-ac6a-b901f2f21210",
   "metadata": {},
   "source": [
    "#### Using autotuned operators to build more efficient kernels\n",
    "\n",
    "A major advantage of TVM's code-generation approach, besides the possibility to apply complex optimization at various abstraction layers, is the degree of freedom in the choice of the `compute` and `schedule` used to represent a given operator. While hand-crafted kernels (see TFLite for Microcontrollers) have to be as generic as possible to support a wide variety of different datatypes, shapes etc. TVM can choose from a number of possible parameterizable implementations for a given operator.\n",
    "\n",
    "The challenge is to find the best implementation alongside a combination of parameters which has the \"best\" performance on a specific target device. An AutoTuner is provided by TVM to automate this process by exploring the search space using a number of exploration and optimization algorithms. As the autotuning procedure is quite time-intensive and requires a complex hardware/software setup, we will not invoke the AutoTuner here. Instead, we have done the autotuing for you. We provide the tuning records (`micro_kws_student_tuning_log_nchw_best.txt`) for the `data/micro_kws_student.tflite` model (see `tvm/data/`). Please use them for the following steps.\n",
    "\n",
    "Add `--tuning-records data/micro_kws_student_tuning_log_nchw_best.txt` to the `tvmc compile` definition to use the tuning logs when compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86dcc32-f28d-4357-acca-843eaede9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p gen\n",
    "!tvmc compile data/micro_kws_student_quantized.tflite --output gen/mlf_tuned.tar \\\n",
    "    --target c --runtime crt --executor-aot-interface-api c \\\n",
    "    --executor aot --executor-aot-unpacked-api 1 --desired-layout NCHW \\\n",
    "    --output-format mlf --pass-config tir.disable_vectorize=1 \\\n",
    "    --pass-config tir.usmp.enable=1 --pass-config tir.usmp.algorithm=hill_climb \\\n",
    "    --tuning-records data/micro_kws_student_tuning_log_nchw_best.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a32bf-90f1-4025-ba0d-8f5f961c6b7e",
   "metadata": {},
   "source": [
    "In a later experiment, we will see the impact of autotuning on the inference speed (performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad5335-cad0-4a77-8bc6-956966128f7a",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "\n",
    "Depending on the used options in the previous step, you will have one of the following artifacts inside `gen/module.tar` or `gen/mlf.tar`:\n",
    "\n",
    "1. A compiled library (shared object) intended to be loaded by a CPUs LLVM runtime (contains `mod.so` (kernels), `mod.json` (graph) and `mod.params`)\n",
    "2. A model library interface (MLF) archive containing the generated kernel in C and runtime required to invoke the model alongside with some additional metadata. For a more detailed explanation of the archive contents/directory structure, see [`mlf_overview.md`](mlf_overview.md)\n",
    "\n",
    "If you want to manually inspect your `.tar` artifact, you can extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094682c0-3c31-4dc8-b313-f50661ca6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared object\n",
    "!mkdir -p gen/module && tar xf gen/module.tar -C gen/module\n",
    "\n",
    "# MLF (Untuned)\n",
    "!mkdir -p gen/mlf && tar xf gen/mlf.tar -C gen/mlf/\n",
    "\n",
    "# MLF (Tuned)\n",
    "!mkdir -p gen/mlf_tuned && tar xf gen/mlf_tuned.tar -C gen/mlf_tuned/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35089cd4-d0c4-491f-ae69-cc3002f1bb54",
   "metadata": {},
   "source": [
    "Feel free to investigate the generated files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23489e3d-3b0b-4601-9c57-16c5edbb81b6",
   "metadata": {},
   "source": [
    "### Testing on host\n",
    "\n",
    "The `tvmc run` subcommand provides an interface to invoke the model on a certain set of targets e.g. CPU (the default option). You can also provide input data to validate if the model outputs match the expectations.\n",
    "\n",
    "Execute the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea18c0-7020-4db0-8f91-723d9fd03f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tvmc run gen/module.tar --fill-mode random --print-time --print-top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee765c0-954d-4168-8081-eaeccb05bc62",
   "metadata": {},
   "source": [
    "The `--print-time` flag is just a benchmark option and can be omitted.\n",
    "\n",
    "Instead of generating random input values, it is possible to provide actual features from the dataset using the `--inputs` option. You can find a script to generate a `.npz` file for TVM in `train/`, which can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664d3ce-d3a1-4730-ab1d-a77719b135f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generated using:\n",
    "# `python ../1_train/wav2features.py /path/to/speech_dataset/no/0137b3f4_nohash_1.wav data/no.npz --output-format npz`\n",
    "# `python ../1_train/wav2features.py /path/to/speech_dataset/yes/0137b3f4_nohash_1.wav data/yes.npz --output-format npz`\n",
    "\n",
    "!echo -e \"no(3):\"\n",
    "!tvmc run gen/module.tar --inputs data/no.npz --print-top 10\n",
    "!echo\n",
    "!echo -e \"yes(2):\"\n",
    "!tvmc run gen/module.tar --inputs data/yes.npz --print-top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348ab60-eb55-43e2-acf6-6ae2a3e707dc",
   "metadata": {},
   "source": [
    "Pay attention to the changing output indices in the first row!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef980b99-0654-4d5b-b9f4-55d5be8f4f1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deployment to a micro-controller\n",
    "\n",
    "While functional verification using actual samples from the dataset is a useful first step, the main goal is to use the generated kernels on a real embedded device to run real-time inference using a microphone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22b3fc-5a7f-4f20-bd85-713b64243499",
   "metadata": {},
   "source": [
    "#### Deploying MicroKWS model to ESP32-C3 dev board using ESP-IDF\n",
    "\n",
    "If we want to deploy out model in a real scenario with our MicroKWS peripherals we need a slightly different approach. Continue with in `3_run` to learn more about the real world overheads of such an application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
